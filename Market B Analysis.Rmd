---
title: " Market Analysis for Target Global"
author: 'Autores: Graciela Rivera, Luis E. Vargas Porras '
output:
  html_document:
    highlight.chooser: yes
    number_sections: yes
    theme: cosmo
    theme.chooser: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE )

```

![](https://image.cnbcfm.com/api/v1/image/107064087-1668607774406-tar.jpg?v=1675257975&w=740&h=416&ffmt=webp&vtcrop=y)

# Fase del entendimiento de negocio: 
***
## Objetivo y criterio de Negocio {.tabset .tabset-fade .tabset-pills} 

### Objetivo

Identificar aquellas transacciones de órdenes de compra de Target Global que sean altamente asociadas entre ellas con el fin de aumentar las promociones de estos productos para el Q1, 2023 y sirva de información para el motor de recomendación de su tienda online. 

### Criterio de Negocio

1)	Aumentar en un 3% las promociones de los productos que tenga una fuerte asociación para el próximo Q3, 2023.
2)	Aumentar las ventas del negocio en 5% para el próximo Q3, 2023, y un 7% para el Q4, 2023 respectivamente.

## Objetivo y criterio de Minería de Datos {.tabset .tabset-fade .tabset-pills} 

### Objetivo

Desarrollar un modelo algorítmico A priori para identificar la fuerza de asociación entre los productos de la tienda Online Target Global.

### Criterios de éxito (desde la perspectiva de minería de datos).
Para nuestro valor mínimo de soporte, especificaremos el 1%, lo que significa que solo se devolverán los artículos cuya frecuencia de aparición en los datos sea al menos del 1%. Y además un 0.85 en la confianza.


# Entendimiento de los Datos
*** 

* Carga Inicial del Data Set Online Retail para el informe inicial de Datos.

```{r}
#Librerías
library(DT)
library(dplyr)
library(ggplot2)
library(knitr)
library(arules)
library(arulesViz)
library(plyr)
```


```{r}
EcommerceData <- read.csv("EcommerceData.csv")
head(EcommerceData)
#View(EcommerceData)
```

* Descripción del Data Set " Online Retail"

El data set cuenta con las siguientes estructura de columnas.

```{r}
head(EcommerceData,10)
```
En cuanto al numero de registros se tienen 541909 observaciones con ocho variables
```{r}
str(EcommerceData)
```
Para tener un resumen descriptivo estadístico de nuestro data set, usamos la función *summary()* el cual nos da el siguiente resultado:
```{r}
summary(EcommerceData)
```


```{r}
ggplot(EcommerceData) +
  aes(x = "", y = UnitPrice) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()
```

```{r}
ggplot(EcommerceData) +
  aes(x = "", y = Quantity) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()
```

Se puede observar que el Data set contiene un total de `541909` filas (transacciones), y cuenta con `ocho` atributos. Además se tienen datos tipo numérico y strings(cadenas).

En cuanto a la calidad de datos, se puede observar que:

1) la columna CustomerID contiene `135080 NA's`, por lo que se deben de eliminar para evitar tener ruido y aumentar la consistencia en los resultados cuando se efectúe el modelo.

2) La columna de fecha **InvoiceDate** es de tipo texto, y queremos que ésta sea de tipo `date`.

3) Para aquellas transacciones que fueron canceladas o devueltas, se identifican con la letra "c" dentro del campo de *InvoiceNo* y por esta razón hay cantidades negativas en la columna "Quantity", las cual se deben de eliminar para el análisis. 

4) Hay records que no necesariamentes son transacciones de productos, sino que se registran cuentas bancarias, fees, devoluciones etc. Lo anterior se puede ver en la columna de *StockCodes* ejemplos códigos D,M,S que representa "Discounts, Manual Imput, Sample.

5) Se tienen precios (UnitPrice) negativos y algunos registros tiene 0, y en la columna de *Descripción* hay registros que estan vacíos.

6) No existe una métrica de ventas dentro del data set, sin embargo se tienen los campos para poder calcularla para nuestro análisis. 

*Este preprocesamiento, se realiza en la "Fase de Preparación"*

Para este momento, conviene realizar una exploración de los datos inicial que ayuden a encontrar nuestros principales hallazgos e hipotésis, donde podríamos llegar a conclusiones que será validadas en los resultados finales. Se hace uso de Tableu para la visualización que incluye plots y gráficos que qyuden a mostrar las características y sugerir/descubrir tendencias interesantes que podrían tener los datos.

[Tableau Proyecto Final](https://public.tableau.com/app/profile/luis.enrique.vargas/viz/ProyectoFinal_16801552299020/Dashboard1)

# Preparación de los datos
*** 

## Cambiar el tipo de dato a Date

```{r }
#datosEcommerce$InvoiceDate <- as.POSIXct(datosEcommerce$InvoiceDate, format="%m/%d/%Y %H:%M")
EcommerceData$InvoiceDate <- as.Date(EcommerceData$InvoiceDate, format="%m/%d/%Y")
str(EcommerceData)
```

## Creación de una nueva métrica

```{r}
EcommerceData$Sales <- EcommerceData$Quantity * EcommerceData$UnitPrice
str(EcommerceData)
```

## Eliminar NA's

```{r}
EcommerceData <- na.omit(EcommerceData)
summary(EcommerceData)
```
## Eliminar ventas negativas.

```{r}
EcommerceData <- EcommerceData[EcommerceData$Sales > 0, ]
summary(EcommerceData)

```

## Eliminar excepciones de *StockCode*

```{r}
EcommerceData <- EcommerceData[-which(EcommerceData$StockCode == 'POST'),]
summary(EcommerceData)

```

```{r}
EcommerceData <- EcommerceData %>% filter(!StockCode %in% c('S','M'))
summary(EcommerceData)
```

```{r}
EcommerceData <- EcommerceData[-which(EcommerceData$StockCode == 'BANK CHARGES'),]
summary(EcommerceData)
```

```{r}
 # se guardan los datos limpios en archivo csv para visualizar en tableau
write.csv(EcommerceData,'EcommerceDataCleaned.csv', quote = FALSE)
```

# Modelado de los datos
*** 

* Modelo de Apriori

Este algoritmo se utiliza sobre bases de datos transaccionales, y permite encontrar de forma eficiente "conjunto de items frecuentes", los cuales sirven de base para generar reglas de asociación. Usualmente se recomienda para bases de datos grandes.

* Modelo de Eclat

ECLAT significa "Equivalence Class Clustering and bottom-up Lattice Traversal". Al igual que el algoritmo de Aproiri es otro método popular de reglas de asociación usado en minería de datos. Es una versión mas eficiente y escalable que el algoritmo de Apriori. Mientras que el algoritmo de Apriori trabaja en el sentido horizontal imitando la búsqueda de Breadth-First en una estructura de árbol, el ECLAT trabaja en el sentido vertical como la búsqueda de Depth-First. Usualmente se recomienda para bases de datos pequeñas y medianas.

## Conversión de los datos originales a formato transaccional

```{r}
# recordar de activar el paquete plyr
# se agrupan las transacciones con igual InvoiceNo y InvoiceDate
# luego se concatena con una coma
EcommerceTransactions<- ddply(EcommerceData,c('InvoiceNo','InvoiceDate'),
                     function(dataf) paste(dataf$Description,collapse = ','))

# se descartan la fecha y el # de facturas para las reglas de asociación
EcommerceTransactions$InvoiceNo<-NULL
EcommerceTransactions$InvoiceDate<-NULL

# se renombra la columna restante como items
colnames(EcommerceTransactions) <- c('Items')

# se guardan los datos como archivo csv en formato transaccion
write.csv(EcommerceTransactions,'EcommerceTransactions.csv', quote = FALSE)
```

## Lectura de los datos en formato transacción

```{r}
# Recordar de activar el paquete arules
# se remueve los items duplicados también
TransactionsLoaded <- read.transactions('EcommerceTransactions.csv',sep=',',format = 'basket', quote = "", rm.duplicates = 1)
```
## Exploración de las transacciones

```{r}
# formato actual de tabla de transacciones
inspect(TransactionsLoaded[1:5])

# recordar aplicar el paquete tidyverse
TibbleTransactions <- as(TransactionsLoaded, Class = "data.frame")

# se convierte el dataframe a tibble para visualizar mejor tabla de datos, 
# sin embargo se sigue usando "TransactionsLoaded" para el resto del análisis
as_tibble(TibbleTransactions) %>% head(10)
```

## Resumen de las transacciones

```{r}
summary(TransactionsLoaded)
```

## Determinar los items más frecuentes

```{r}
itemFrequencyPlot(TransactionsLoaded,topN=10,type='absolute',main="Items más frecuentes")
```

## Definición de parámetros para modelos

Dado los objetivos de minería planteados en el anteproyecto, se requiere un soporte mínimo del 1%, y un mínimo de confianza del 0.85

## Generación de reglas para cada modelo

```{r}
eclatItemsets <- eclat(TransactionsLoaded, parameter=list(supp=0.005, maxlen=10))
tmpEclatRules <- ruleInduction(eclatItemsets, confidence = 0.85)

# eliminar reglas que son subconjuntos de otras 
eclatSubconjuntos<- which(colSums(is.subset(tmpEclatRules,tmpEclatRules))>1)
eclatRules<- tmpEclatRules[-eclatSubconjuntos]

#inspect(eclatRules[1:10])
# Ordena items de acuerdo al soporte
inspect(sort(eclatRules,by='support',decreasing = TRUE)[1:20])
```

```{r}
tmpAprioriRules <- apriori(TransactionsLoaded, parameter=list(supp=0.005, conf=0.85, maxlen=10))

# eliminar reglas que son subconjuntos de otras 
aprioriSubconjuntos<- which(colSums(is.subset(tmpAprioriRules,tmpAprioriRules))>1)
aprioriRules<- tmpAprioriRules[-aprioriSubconjuntos]

#inspect(aprioriRules[1:10])
# Ordena items de acuerdo al soporte
inspect(sort(aprioriRules,by='support',decreasing = TRUE)[1:20])
```

## Exportar reglas de ambos modelos

```{r}
write(eclatRules,file='eclatRules.csv',sep=',',quote=F,row.names=F)
write(aprioriRules,file='aprioriRules.csv',sep=',',quote=F,row.names=F)
```
# Visualización de resultados
***

## Top 5 reglas según la confianza usando ECLAT

```{r}
# recordar activar arulesViz
eclatTop5Rules<- head(eclatRules,n=5,by='confidence')
plot(eclatTop5Rules,method = 'graph',engine = 'htmlwidget')
```

## Top 5 reglas según la confianza usando Apriori

```{r}
aprioriTop5Rules<- head(aprioriRules,n=5,by='confidence')
plot(aprioriTop5Rules,method = 'graph',engine = 'htmlwidget')
```

## Top 10 items de acuerdo a la confianza usando ECLAT 

```{r}
eclatTop10Rules<-head(eclatRules,n=10,by='confidence')
plot(eclatTop10Rules,method ='paracoord')
```

## Top 10 items de acuerdo a la confianza usando Apriori 

```{r}
aprioriTop10Rules<-head(aprioriRules,n=10,by='confidence')
plot(aprioriTop10Rules,method ='paracoord')
```


# Desempeño de modelos
***

**Tiempos de ejecución por modelo**: Aproximadamente 0.16s para el apriori, mientras que para el ECLAT es de alrededor de 0.88s

**Reglas de asociación generadas por modelo**: 388 para el apriori, y 4066 para el ECLAT. 


# Conclusiones
***

* **A nivel de asociación**: el modelo de Eclat muestra que se generan más reglas con respecto al Apriori. Sin embargo, a la hora de eliminar las reglas que son subconjuntos de otras se obtiene el mismo número de reglas entre modelos. Lo cual no marca ninguna diferencia significativa entre ambos modelos. 

* **A nivel de tiempo de ejecución**: el modelo de Apriori es muchas rapido que el ECLAT. 

* Los productos con mayor soporte son los siguientes: {PINK REGENCY TEACUP AND SAUCER, ROSES REGENCY TEACUP AND SAUCER} => {GREEN REGENCY TEACUP AND SAUCER}. Estos se tendrian que contemplar para el motor de recomendación de la tienda online.

* De acuerdo al análisis de Apriori o Eclat los primeros 10 Itemsets son los que tiene un soporte mayor al 1% y una confianza mayor al 0.85. Los cuales se puede utilizar para definir una estrategia de venta mas eficiente.


